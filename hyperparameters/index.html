<html lang="en">
	</head>
		<meta charset="utf-8">
		<meta name="description" content="Munachiso Nwadike's research page">
		<meta name="author" content="Munachiso Nwadike">
		<meta name="viewport" content="width=device-width, initial-scale=1.0">
		
		<link rel="preconnect" href="https://fonts.gstatic.com">
		<link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;700&family=Open+Sans&display=swap" rel="stylesheet">
		<style>   
			.title-div { 
 				float: top;     
				font-family: 'Montserrat';
				font-size: 40px;
				background-color: white;   
				border-bottom: 0px solid black;  
				width: 1200px;
				margin: auto;
				text-align: center;
			} 
			.section-div { 
				float: center; 
				width: 710px;
				margin: auto;
			} 
			.matplotlib-div { 
				float: center; 
				width: 900px;
				margin: auto;
			}  
			p {
				font-family: 'Open Sans';
				font-size: 16px;
				text-align: justify;
			} 
			img {
				max-width:100%;
			} 
		</style>
		<script type="text/x-mathjax-config">
		    MathJax.Hub.Config({
		      tex2jax: {
			skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
			inlineMath: [['$','$']]
		      }
		    });
		</script>
		<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script> 
 		<title> Hyperparameter Tuning Project Page </title>
	</head>
    
	<body>
		<div class = "title-div" >
			<br><br> 
			<b>Bayesian Optimisation <br> for Efficient ML Pipeline Hyperparameter Tuning <br> Under a Cost Budget </b>

		</div>
		<div class = "title-div"  style="font-family:'Montserrat';font-size:20px;color:#209CEE;" >
			<br>
			<b>
			<a style="color:#209CEE;" href="https://munachisonwadike.github.io/">Munachiso S. Nwadike</a>,
			Navish Kumar, 
			Kirill Vishniakov,  
			<a style="color:#209CEE;" href="https://willieneis.github.io/">Willie Neiswanger</a>,
			<a style="color:#209CEE;" href="https://www.andrew.cmu.edu/user/kunz1/index.html">Kun Zhang</a>,
			<a style="color:#209CEE;" href="https://sites.google.com/site/hoqirong/home">Qirong Ho</a>,
			<a style="color:#209CEE;" href="http://www.cs.cmu.edu/~epxing/">Eric Xing</a>
			</b>
		</div>
		<div class = "title-div"  style="font-family: 'Montserrat'; font-size: 30px;" >
			</br>
			<a href="https://munachisonwadike.github.io/"> 
			<button style="background-color:#363636;border-color:#363636;color:white;font-size: 25px;padding: 5px 10px;border-radius: 12px;">üìù Paper</button> 
			</a>
		</div> 

		<div class = "title-div"  style="font-family: 'Montserrat'; font-size: 16px;color:#209CEE" >
			<br>
			<b>In Publication, Coming Soon.</b>
		</div> 

		<div class = "title-div"  style="font-family: 'Montserrat'; font-size: 18px;color:#4A4A4A;width:800px" >
			<br><br><br>
			<b>One Sentence Summary: <span style="color:#38761D">Bayesian Optimisation </span>, 
				<span style="color:#38761D">Pipelines</span> setting in ML, Learned Cost, <span style="color:#38761D">Memoisation</span></b>
			<br>
			<br>
			<br>
			<br> 
		</div> 
		
		<div class="section-div" style="float:center;width:1000;height:700 ">
			<div class = "section-div" style="float:left;width:500;height:700"> 
				<center><p style="font-family: 'Montserrat'; font-size: 40px; text-align: center;">Overview</p> </center>
				<p>Bayesian optimization (BO) is often used for 
					hyperparameter tuning, due to its ability to optimize black-box functions efficiently
					Efficiency is often measured in terms of number of iterations required for convergence. 
					However, at each iteration of the BO algorithm, different
					hyperparameters may incur different costs (i.e. time units required to
					perform a query). Thus, measuring covergence speed in terms of number of iterations,
					may not be representative of the true cost incurred by a given BO acquisition function. Furthermore,
					most BO acqusition functions are often tailored to tune the hyperparameters of the ML model alone. For example, when tuning a random forest classifier model, assume that we have access to an oracle, which tells us that the ideal <i>max depth</i> hyperparameter is 100. We would like our BO algorithm to achieve a tradeoff between a small max depth hyperparameter (say 10), which may result in a low accuracy, at a low latency, and a high max depth (say 10000), which could potentially result in a high accuracy, at high latency. However, this random forest classifier may depend on a preprocessing step such as TF-IDF vectorisation, which also bears tunable hyperparameters. Indeed, ML models are often dependent on one or more preprocessing and postprocessing steps, algother forming an <b><i>ML pipeline</b></i> (see visualisation to the left). Our project seeks to develop BO algorithms that perform cost aware optimisation tailored to ML pipelines hyparameters in an end-to-end fashion.
					</p>
			</div>
			<div class = "section-div" style="float:right;width:450;height:700">  
				<center><p style="font-family: 'Montserrat'; font-size: 40px; text-align: center;">Example</p> </center>  
				<center><img src="problem_visualisation.PNG" alt="problem Visualisation" style="margin-top:10px;max-width:100%;">  <center>
				<p style="font-size:14px;width:430;"> A visualisation of an ML pipeline for the object detection task. 
				Here we can have a preprocessing step such as image resizing or image deblurring,
				whereby the resize width/height, and gaussian kernel size may be hyperparameters. Postprocessing can include NMS
				threshold or choise of NMS technique.</p>
			</div> 
		</div>	

		<div class = "section-div"  style="float:center;width:850;">  

				<center><p style= "font-family: 'Montserrat'; font-size: 40px; text-align: center;">Heterogenous Cost</p> <center>
				<!-- <center><img src="memoisation.PNG" alt="Memoisation" style="max-width:85%;"> </center> -->

				<p>	A multi-stage ML pipeline will have multiple cost subspaces. 
					Consider, for example, a 3-stage pipeline with 2 hyperparameters per stage. 
					We can set the cost functions for the 3 stages as $|\sin(x_1^2)+\cos(x_1+x_2)|$, $|\sin(x_3)\cdot \cos(10+x_4)|$,
					and $|\cos(x_5/x_6) \cdot \sin(4-x_5x_6)\cdot \sin(0.5\cdot x_5) |$ respectively. Define the hyperparameters
					*****INSET**
					$x = [x_1,\cdots,x_6] \in \mathcal{X} = [-5,5]^6 \subset \mathbb{R}^6$. 
					In the image below, these three functions are visualised on the left hand side. </p>

					<center><img src="cost_landscape.PNG" alt="Cost" style="margin-top:10px;max-width:70%;"> </center>
				<p>
					The overall cost function of the pipeline is therefore $|\sin(x_1^2)+\cos(x_1+x_2)|$ + $|\sin(x_3)\cdot \cos(10+x_4)|$ + $|\cos(x_5/x_6) \cdot \sin(4-x_5x_6)\cdot \sin(0.5\cdot x_5) |$. The addition of the costs 
					is the direct result of composition of pipelines stages. The heatmap of the total cost is represented 
					in the top left corner of the above image. Note that while this is a function in $\mathbb{R}^6$, we 
					represent it in two-dimensions for ease of viewing. 
					The  objective function is defined as $\mathrm{obj}(x) = |x_1^2 + x_2^2 + x_3^2 + x_4^2 + x_5^2 + x_6|$, 
					also represented in 2D.
 					Observe that the high cost and low cost regions occur in different parts of the space from the high and low objective function regions. 
					Our goals, with ournovel BO algorithm for ML pipelines, is to is to minimise total cost, while optimising the objective function. </p>

		</div>

		<div class = "section-div"  style="float:center;width:850;"> 
			<br><br><br>
			<center><p style= "font-family: 'Montserrat'; font-size: 40px; text-align: center;">Methodology</p> <center>
			

			<p>We usually think of just the model stage, as visualised in the image above. Now let us think about
			individual hyperparameter, as in the image below: what do you see? The answer is ... </p>
			<center><img src="3_stage_ppln.PNG" alt="Cost" style="max-width:53%;" ></center> 

			<p>When performing hyperparameter search in an ML pipeline
				setting, we can cache hyperparameter combinations from earlier stages of a pipeline, once
				queried, to be reused with hyperparameter combinations of latter stages in a pipeline, for
				a more effecient search. This cache-and-reuse is referred to as memoisation. We propose
				a new acquisition function for BO, referred to as EEIPU, that is uniquely suited to the
				pipeline setting. Furthermore, we explore techniques regarding the combination of EEIPU,
				with memoised hyperparameter search.</p>

			<center><img src="memoisation.PNG" alt="Memoisation" style="max-width:80%;"> </center>
		
		</div>
		
		<div class = "section-div" style="float:center;width:1050;"> 
			<br><br> 	
			<center><p style="font-family: 'Montserrat'; font-size: 40px; text-align: center;">Experiments</p> </center>  
			<center><img src="norm3d_2.png" alt="Cost" style="max-width:95%;" ></center> 
			<center><img src="branin4d.png" alt="Cost" style="max-width:95%;" ></center> 
			
		</div>
		<div class = "section-div" style="float:center;width:850;"> 
			<br><br>
			<p>These are the final Words</p>
			<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>
		</div> 
	</body>
</html>
