<html lang="en">
	</head>
		<meta charset="utf-8">
		<meta name="description" content="Munachiso Nwadike's research page">
		<meta name="author" content="Munachiso Nwadike">
		<meta name="viewport" content="width=device-width, initial-scale=1.0">
		
		<link rel="preconnect" href="https://fonts.gstatic.com">
		<link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;700&family=Open+Sans&display=swap" rel="stylesheet">
		<style>   
			.title-div { 
 				float: top;     
				font-family: 'Montserrat';
				font-size: 40px;
				background-color: white;   
				border-bottom: 0px solid black;  
				width: 1200px;
				margin: auto;
				text-align: center;
			} 
			.section-div { 
				float: center; 
				width: 710px;
				margin: auto;
			} 
			.matplotlib-div { 
				float: center; 
				width: 900px;
				margin: auto;
			}  
			p {
				font-family: 'Open Sans';
				font-size: 16px;
				text-align: justify;
			} 
			img {
				max-width:100%;
			} 
		</style>
		<script type="text/x-mathjax-config">
		    MathJax.Hub.Config({
		      tex2jax: {
			skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
			inlineMath: [['$','$']]
		      }
		    });
		</script>
		<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script> 
 		<title> Hyperparameter Tuning Project Page </title>
	</head>
    
	<body>
		<div class = "title-div" >
			<br><br> 
			<b>Bayesian Optimisation <br> for Efficient ML Pipeline Hyperparameter Tuning <br> Under a Cost Budget </b>

		</div>
		<div class = "title-div"  style="font-family:'Montserrat';font-size:20px;color:#209CEE;" >
			<br>
			<b>
			<a style="color:#209CEE;" href="https://munachisonwadike.github.io/">Munachiso S. Nwadike</a>,
			Navish Kumar, 
			Kirill Vishniakov,  
			<a style="color:#209CEE;" href="https://willieneis.github.io/">Willie Neiswanger</a>,
			<a style="color:#209CEE;" href="https://www.andrew.cmu.edu/user/kunz1/index.html">Kun Zhang</a>,
			<a style="color:#209CEE;" href="https://sites.google.com/site/hoqirong/home">Qirong Ho</a>,
			<a style="color:#209CEE;" href="http://www.cs.cmu.edu/~epxing/">Eric Xing</a>
			</b>
		</div>
		<div class = "title-div"  style="font-family: 'Montserrat'; font-size: 30px;" >
			</br>
			<a href="https://munachisonwadike.github.io/"> 
			<button style="background-color:#363636;border-color:#363636;color:white;font-size: 25px;padding: 5px 10px;border-radius: 12px;">üìù Paper</button> 
			</a>
		</div> 

		<div class = "title-div"  style="font-family: 'Montserrat'; font-size: 16px;color:#209CEE" >
			<br>
			<b>In Publication, Coming Soon.</b>
		</div> 

		<div class = "title-div"  style="font-family: 'Montserrat'; font-size: 18px;color:#4A4A4A;width:800px" >
			<br><br><br>
			<b>One Sentence Summary: <span style="color:#38761D">Bayesian Optimisation </span>, 
				<span style="color:#38761D">Pipelines</span> setting in ML, Learned Cost, <span style="color:#38761D">Memoisation</span></b>
			<br>
			<br>
			<br>
			<br> 
		</div> 
		
		<div class="section-div" style="float:center;width:1000;height:550 ">
			<div class = "section-div" style="float:left;width:500;height:550"> 
				<center><p style="font-family: 'Montserrat'; font-size: 40px; text-align: center;">Abstract</p> </center>
				<p>Bayesian optimization (BO) is particularly useful for 
					hyperparameter tuning. At each iteration of BO, different
					hyperparameters may incur different costs, i.e time units required to
					perform a query. Thus measuring covergence speed in terms of number of iterations,
					may not be representative of the true cost of a hyperparameter. For instance, 
					different learning ratesin a model training step may result in different convergence times.
					Training hyperparameters alone may not be sufficient to train. 
					For eaxmple preprocessing and postprocessing steps form part of the ML pipeline. 
					When performing hyperparameter search in an ML pipeline
					setting, we can cache hyperparameter combinations from earlier stages of a pipeline, once
					queried, to be reused with hyperparameter combinations of latter stages in a pipeline, for
					a more effecient search. This cache-and-reuse is referred to as memoisation. We propose
					a new acquisition function for BO, referred to as EEIPU, that is uniquely suited to the
					pipeline setting. Furthermore, we explore techniques regarding the combination of EEIPU,
					with memoised hyperparameter search.</p>
			</div>
			<div class = "section-div" style="float:right;width:450;height:550">  
				<center><p style="font-family: 'Montserrat'; font-size: 40px; text-align: center;">Example</p> </center>  
				<center><img src="problem_visualisation.PNG" alt="Cost" style="margin-top:10px;max-width:90%;">  <center>
			</div> 
		</div>	

		<div class = "section-div"  style="float:center;width:850;"> 
				<center><p style= "font-family: 'Montserrat'; font-size: 40px; text-align: center;">Heterogenous Cost</p> <center>
				<!-- <center><img src="memoisation.PNG" alt="Memoisation" style="max-width:85%;"> </center> -->
				<center><img src="cost_landscape.PNG" alt="Cost" style="margin-top:10px;max-width:70%;"> </center>
				<p>Consider $x = [x_1,\cdots,x_6] \in \mathcal{X} = [-5,5]^6 \subset \mathbb{R}^6$. 
					For  objective function in (a), $\mathrm{obj}(x) = |x_1^2 + x_2^2 + x_3^2 + x_4^2 + x_5^2 + x_6|$. 
					The corresponding $\mathrm{cost}(x)$ in (b) is the sum of costs in (c), (d), and (e). In (c) is the cost in $|\sin(x_1^2)+\cos(x_1+x_2)|$. 
					In (d)  the cost function is $|\sin(x_3)\cdot \cos(10+x_4)|$ and in (e) the cost function is $|\cos(x_5/x_6) \cdot \sin(4-x_5x_6)\cdot \sin(0.5\cdot x_5) |$. 
					Note that while the functions in (a) and (b) are 6-dimensional, we represent them in 2D. 
					The figure (b) infdicates the high cost and low cost regions occur in different parts of the space from the high and low objective function regions. 
					Our work is particular concerned with settings where cost and objective are uncorrelated.
					Our goals, with multi-stage ML pipelines, is to minimise total cost, while optimising the objective function shown in (a). </p>

				<p>We usually think of just the model stage, as visualised in the image above. Now let us think about
				individual hyperparameter, as in the image below: what do you see? The answer is ... </p>

				<p>The question is how we can address the problem of .. </p>

		</div>

		<div class = "section-div"  style="float:center;width:850;"> 

				<center><p style= "font-family: 'Montserrat'; font-size: 40px; text-align: center;">Methodology</p> <center>
				

				<p>We usually think of just the model stage, as visualised in the image above. Now let us think about
				individual hyperparameter, as in the image below: what do you see? The answer is ... </p>
				<center><img src="3_stage_ppln.PNG" alt="Cost" style="max-width:55%;" ></center> 

				<p>The question is how we can address the problem of .. </p>

				<!-- <center><img src="memoisation.PNG" alt="Memoisation" style="max-width:70%;"> </center> -->

		</div>
		
		<div class = "section-div" style="float:center;width:1050;"> 
			<br><br> 	
			<center><p style="font-family: 'Montserrat'; font-size: 40px; text-align: center;">Experiments</p> </center>  
			<center><img src="norm3d_2.png" alt="Cost" style="max-width:90%;" ></center> 
			<center><img src="branin4d.png" alt="Cost" style="max-width:90%;" ></center> 
			

			<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>
		</div> 
	</body>
</html>
