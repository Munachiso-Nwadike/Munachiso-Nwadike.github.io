<html lang="en">
	</head>
		<meta charset="utf-8">
		<meta name="description" content="Munachiso Nwadike's research page">
		<meta name="author" content="Munachiso Nwadike">
		<meta name="viewport" content="width=device-width, initial-scale=1.0">
		
		<link rel="preconnect" href="https://fonts.gstatic.com">
		<link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;700&family=Open+Sans&display=swap" rel="stylesheet">
		<style>   
			.title-div { 
 				float: top;     
				font-family: 'Montserrat';
				font-size: 40px;
				background-color: white;   
				border-bottom: 0px solid black;  
				width: 1200px;
				margin: auto;
				text-align: center;
			} 
			.section-div { 
				float: center; 
				width: 710px;
				margin: auto;
			} 
			.matplotlib-div { 
				float: center; 
				width: 900px;
				margin: auto;
			}  
			p {
				font-family: 'Open Sans';
				font-size: 16px;
				text-align: justify;
			} 
			img {
				max-width:100%;
			} 
		</style>
		<script type="text/x-mathjax-config">
		    MathJax.Hub.Config({
		      tex2jax: {
			skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
			inlineMath: [['$','$']]
		      }
		    });
		</script>
		<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script> 
 		<title> Hyperparameter Tuning Project Page </title>
	</head>
    
	<body>
		<div class = "title-div" >
			<br><br> 
			<b>Bayesian Optimisation <br> for Efficient ML Pipeline Hyperparameter Tuning <br> Under a Cost Budget </b>

		</div>
		<div class = "title-div"  style="font-family:'Montserrat';font-size:20px;color:#209CEE;" >
			<br>
			<b>
			<a style="color:#209CEE;" href="https://munachisonwadike.github.io/">Munachiso S. Nwadike</a>,
			Navish Kumar, 
			Kirill Vishniakov,  
			<a style="color:#209CEE;" href="https://willieneis.github.io/">Willie Neiswanger</a>,
			<a style="color:#209CEE;" href="https://www.andrew.cmu.edu/user/kunz1/index.html">Kun Zhang</a>,
			<a style="color:#209CEE;" href="https://sites.google.com/site/hoqirong/home">Qirong Ho</a>,
			<a style="color:#209CEE;" href="http://www.cs.cmu.edu/~epxing/">Eric Xing</a>
			</b>
		</div>
		<div class = "title-div"  style="font-family: 'Montserrat'; font-size: 30px;" >
			</br>
			<a href="https://munachisonwadike.github.io/"> 
			<button style="background-color:#363636;border-color:#363636;color:white;font-size: 25px;padding: 5px 10px;border-radius: 12px;">üìù Paper</button> 
			</a>
		</div> 

		<div class = "title-div"  style="font-family: 'Montserrat'; font-size: 16px;color:#209CEE" >
			<br>
			<b>In Publication, Coming Soon.</b>
		</div> 

		<div class = "title-div"  style="font-family: 'Montserrat'; font-size: 18px;color:#4A4A4A;width:800px" >
			<br><br><br>
			<b>One Sentence Summary: <span style="color:#38761D">Bayesian Optimisation </span>, 
				<span style="color:#38761D">Pipelines</span> setting in ML, Learned Cost, <span style="color:#38761D">Memoisation</span></b>
			<br>
			<br>
			<br>
			<br> 
		</div> 
		
		<div class="section-div" style="float:center;width:1000;height:700 ">
			<div class = "section-div" style="float:left;width:500;height:700"> 
				<center><p style="font-family: 'Montserrat'; font-size: 40px; text-align: center;">Overview</p> </center>
				<p>Bayesian optimization (BO) is often used for 
					hyperparameter tuning, due to its ability to optimize black-box functions efficiently
					Efficiency is often measured in terms of number of iterations required for convergence. 
					However, at each iteration of the BO algorithm, different
					hyperparameters may incur different costs (i.e. time units required to
					perform a query). Thus, measuring covergence speed in terms of number of iterations,
					may not be representative of the true cost incurred by a given BO acquisition function. Furthermore,
					most BO acqusition functions are often tailored to tune the hyperparameters of the ML model alone. For example, when tuning a random forest classifier model, assume that we have access to an oracle, which tells us that the ideal <i>max depth</i> hyperparameter is 100. We would like our BO algorithm to achieve a tradeoff between a small max depth hyperparameter (say 10), which may result in a low accuracy, at a low latency, and a high max depth (say 10000), which could potentially result in a high accuracy, at high latency. However, this random forest classifier may depend on a preprocessing step such as TF-IDF vectorisation, which also bears tunable hyperparameters. Indeed, ML models are often dependent on one or more preprocessing and postprocessing steps, as part of an <b><i>ML pipeline</b></i> (see visualisation on the right). We propose a novel BO acquisition function to ML pipeline hyparameter tuning, and demonstrate its experimental cost-efficiency.
					</p>
			</div>
			<div class = "section-div" style="float:right;width:450;height:700">  
				<center><p style="font-family: 'Montserrat'; font-size: 40px; text-align: center;">Example</p> </center>  
				<center><img src="problem_visualisation.PNG" alt="problem Visualisation" style="margin-top:10px;max-width:100%;">  <center>
				<br>
				<p style="font-size:14px;width:430;"> Visualisation of an ML pipeline for the object detection task. 
				Here, the preprocessing step may include image resizing and deblurring. The 
				resize width/height, and gaussian kernel represent preprocessing stage hyperparameters. Postprocessing hyperparameters may include NMS threshold.</p>
			</div> 
		</div>	

		<div class = "section-div"  style="float:center;width:850;">  

				<center><p style= "font-family: 'Montserrat'; font-size: 40px; text-align: center;">Heterogenous Cost</p> <center>
				<!-- <center><img src="memoisation.PNG" alt="Memoisation" style="max-width:85%;"> </center> -->

				<p>	A multi-stage ML pipeline will have multiple cost subspaces. 
					Consider, for example, the below 3-stage pipeline with 2 hyperparameters per stage. 
					$[x_1, x_2]$, $[x_3, x_4]$, and $[x_5, x_6]$ represent the hyperparameters for stage 1, 2 and 3 respectively.
					Meanwhile, the cost functions for the 3 stages are represented by $|\sin(x_1^2)+\cos(x_1+x_2)|$, $|\sin(x_3)\cdot \cos(10+x_4)|$,
					and $|\cos(x_5/x_6) \cdot \sin(4-x_5x_6)\cdot \sin(0.5\cdot x_5) |$ respectively. If we consider the heatmaps of cost values for each stage, wherein the hyperparameters are restricted to
					$[-5,5]$, the cost function spaces will appears as shown on the right hand side of the below image. </p>

					<center><img src="cost_landscape.PNG" alt="Cost" style="margin-top:10px;max-width:70%;"> </center>

				<p>
					The overall cost function of the pipeline is therefore $|\sin(x_1^2)+\cos(x_1+x_2)|$ + $|\sin(x_3)\cdot \cos(10+x_4)|$ + $|\cos(x_5/x_6) \cdot \sin(4-x_5x_6)\cdot \sin(0.5\cdot x_5) |$. The addition of the costs 
					is the direct result of composition of pipelines stages. The heatmap of the total cost is represented 
					in the top left corner of the above image. Note that while this is a function in $\mathbb{R}^6$, we 
					represent it in two-dimensions for ease of viewing. 
					The  objective function is defined as $\mathrm{obj}(x) = |x_1^2 + x_2^2 + x_3^2 + x_4^2 + x_5^2 + x_6|$, 
					also represented in 2D.
 					Observe that the high cost and low cost regions occur in different parts of the space from the high and low objective function regions. 
					Our goals, with ournovel BO algorithm for ML pipelines, is to is to minimise total cost, while optimising the objective function. </p>

		</div>

		<div class = "section-div"  style="float:center;width:850;"> 
			<br><br><br>
			<center><p style= "font-family: 'Montserrat'; font-size: 40px; text-align: center;">Methodology</p> <center>
			

			<!-- <p>We usually think of just the model stage, as visualised in the image above. Now let us think about
			individual hyperparameter, as in the image below: what do you see? The answer is ... </p>
			<center><img src="3_stage_ppln.PNG" alt="Cost" style="max-width:53%;" ></center> --> 

			<p>When performing hyperparameter search in an ML pipeline
				setting, we can cache hyperparameter combinations from earlier stages of a pipeline, once
				queried, to be reused with hyperparameter combinations of latter stages in a pipeline, for
				a more effecient search. This cache-and-reuse is referred to as memoisation. For example, to the left of the image below, we have a 3-stage pipeline where $x_1=1.2$, $x_2=4.0$, and $x_3=1.0$, 
				are the hyperparameters for the 3 respective pipeline stages. 
				The costs associated with the hyperparameter at each stage, is shown in orange, so for instance, the cost for the first stage is 5.1, while that of the last stage is 2.0 </p>

			<center><img src="memoisation.PNG" alt="Memoisation" style="max-width:80%;"> </center>
			<p> We propose
				a new acquisition function for BO, referred to as EEIPU, that is uniquely suited to the
				pipeline setting.  We show how it can be combined with memoised hyperparameter search for efficient BO. 
				For instance, to the right of the image below, we have what is known as a "prefix pool", resulting from the single queried hyperparameter combination on the left. 
				This prefix pool has two entries: the first is a hyperparameter combination where only the first stage is fixed at 1.2, and the second is a hyperparameter combination where the first 2 stages are fixed. Our EEIPU acquisition function is built to bias towards regions of the space which have already been queried. For the first entry, we would only need to query within a square region to find optimal values for $x_2$ and $x_3$, while for the secon entry, we would need to perform a <i>line search</i>. Entries in our prefix pool represent a unique opportunity to achieve cost minimisation, since previously queried hyperparameter combinations for a  stage can be discounted from the total cost for all stages. </p>
		</div>
		
		<div class = "section-div" style="float:center;width:1050;"> 
			<br><br> 	
			<center><p style="font-family: 'Montserrat'; font-size: 40px; text-align: center;">Experiments</p> </center>  
		</div>
		<div class = "section-div" style="float:center;width:850;">  
			<p>If indeed our algorithm worked, we would expect to see that our EEIPU function would perform identically to EIPU. This is precisely what happens. EIPU has the advantage of having an oracle available to it. However EEIPU does not. All methods outpuerform random search. Typical expected improvement, which does not factor in cost at all, would perform well in terms of 
			the number of iterations, but underperform in terms of cumulative cost, this is precisely the nature of our observed results.</p>
		</div>
		<div class = "section-div" style="float:center;width:1050;"> 
			<center><img src="norm3d_2.png" alt="Cost" style="max-width:95%;" ></center> 
			<center><img src="branin4d.png" alt="Cost" style="max-width:95%;" ></center> 	
		</div>
		<div class = "section-div" style="float:center;width:850;"> 
			<br><br>
			<p>If indeed our algorithm worked, we would expect to see that our EEIPU function would perform identically to EIPU. This is precisely what happens. EIPU has the advantage of having an oracle available to it. However EEIPU does not. All methods outpuerform random search. Typical expected improvement, which does not factor in cost at all, would perform well in terms of 
			the number of iterations, but underperform in terms of cumulative cost, this is precisely the nature of our observed results.</p>
			<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>
		</div> 
	</body>
</html>
